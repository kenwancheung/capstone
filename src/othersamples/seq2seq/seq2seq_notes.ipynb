{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.4.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\t.cri.kcheung\\\\capstone\\\\src\\\\othersamples\\\\seq2seq'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2728: DtypeWarning: Columns (18,20,35,54,63,66,163,175,178,179,181,182,184,185,187,188,190,191,193,194,196,197,199,200,202,203,205,206,208,209,211,212,214,215,217,218,220,221,223,224,226,227,229,230,232,233,235,236,238,239,241,242,244,245,247,248,250,251,253,254,256,257,259,260,262,263,265,266,268,269,271,272,274,275,277,278,280,281,283,284,286,287,289,290,292,293,295,296,298,299,301,302,304,305,307,308,310,311,313,314,316,317,319,320,322,323) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# notes = pd.read_csv(\"Z:/notes v2/cohort1_deid_df.csv\")\n",
    "notes = pd.read_csv(\"Z:/final_data/cohort1_final_data.csv\")\n",
    "# notes = pd.read_csv(\"gpfs/data/ildproject-share/final_data/cohort1_final_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>findings</th>\n",
       "      <th>impressions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Left-sided AICD are unchanged. Sternal wires a...</td>\n",
       "      <td>No acute cardiopulmonary process.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Increasing right basilar opacity highly sugges...</td>\n",
       "      <td>Increasing right basilar opacity highly sugges...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            findings  \\\n",
       "0                                                NaN   \n",
       "1  Left-sided AICD are unchanged. Sternal wires a...   \n",
       "2  Increasing right basilar opacity highly sugges...   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                         impressions  \n",
       "0                                                NaN  \n",
       "1                  No acute cardiopulmonary process.  \n",
       "2  Increasing right basilar opacity highly sugges...  \n",
       "3                                                NaN  \n",
       "4                                                NaN  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes = notes[['findings','impressions']]\n",
    "notes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "findings       40179\n",
       "impressions    40018\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove null values and unneeded features\n",
    "notes = notes.dropna()\n",
    "# notes = notes.drop(['Unnamed: 0','ScanType','ScanDate','ClinInfo','Technique','Comparison',\n",
    "#                         'ElecSig','Patient','RawText'], 1)\n",
    "notes = notes.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>findings</th>\n",
       "      <th>impressions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Left-sided AICD are unchanged. Sternal wires a...</td>\n",
       "      <td>No acute cardiopulmonary process.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Increasing right basilar opacity highly sugges...</td>\n",
       "      <td>Increasing right basilar opacity highly sugges...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unremarkable cardiac and mediastinal silhouett...</td>\n",
       "      <td>Mild diffuse interstitial disease, previously ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Increase in diffuse pulmonary opacities. Small...</td>\n",
       "      <td>Increase in diffuse pulmonary opacities which ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Diffuse nonspecific interstitial opacity, not ...</td>\n",
       "      <td>Diffuse nonspecific interstitial disease of un...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            findings  \\\n",
       "0  Left-sided AICD are unchanged. Sternal wires a...   \n",
       "1  Increasing right basilar opacity highly sugges...   \n",
       "2  Unremarkable cardiac and mediastinal silhouett...   \n",
       "3  Increase in diffuse pulmonary opacities. Small...   \n",
       "4  Diffuse nonspecific interstitial opacity, not ...   \n",
       "\n",
       "                                         impressions  \n",
       "0                  No acute cardiopulmonary process.  \n",
       "1  Increasing right basilar opacity highly sugges...  \n",
       "2  Mild diffuse interstitial disease, previously ...  \n",
       "3  Increase in diffuse pulmonary opacities which ...  \n",
       "4  Diffuse nonspecific interstitial disease of un...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] dimensions of notes (27284, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"[info] dimensions of notes\",notes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] length of unique findings 26590\n"
     ]
    }
   ],
   "source": [
    "print(\"[info] length of unique findings\",len(np.unique(notes.findings)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords = True):\n",
    "    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\n",
    "    \n",
    "    # Convert words to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace contractions with their longer forms \n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "    \n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    return(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries are complete.\n",
      "Texts are complete.\n"
     ]
    }
   ],
   "source": [
    "# remove stopwords, keepo for summary\n",
    "# Clean the summaries and texts\n",
    "clean_summaries = []\n",
    "for summary in notes.impressions:\n",
    "    clean_summaries.append(clean_text(summary, remove_stopwords=False))\n",
    "print(\"Summaries are complete.\")\n",
    "\n",
    "clean_texts = []\n",
    "for text in notes.findings:\n",
    "    clean_texts.append(clean_text(text))\n",
    "print(\"Texts are complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(count_dict, text):\n",
    "    '''Count the number of occurrences of each word in a set of text'''\n",
    "    for sentence in text:\n",
    "        for word in sentence.split():\n",
    "            if word not in count_dict:\n",
    "                count_dict[word] = 1\n",
    "            else:\n",
    "                count_dict[word] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocabulary: 9961\n"
     ]
    }
   ],
   "source": [
    "# Find the number of times each word was used and the size of the vocabulary\n",
    "word_counts = {}\n",
    "\n",
    "count_words(word_counts, clean_summaries)\n",
    "count_words(word_counts, clean_texts)\n",
    "            \n",
    "print(\"Size of Vocabulary:\", len(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings: 417195\n"
     ]
    }
   ],
   "source": [
    "# Load Conceptnet Numberbatch's (CN) embeddings, similar to GloVe, but probably better \n",
    "# (https://github.com/commonsense/conceptnet-numberbatch)\n",
    "embeddings_index = {}\n",
    "with open('Z:/helper/numberbatch-en-17.06.txt', encoding='utf-8') as f:\n",
    "# with open('gpfs/data/ildproject-share/helper/numberbatch-en-17.06.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = embedding\n",
    "\n",
    "print('Word embeddings:', len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words missing from CN: 615\n",
      "Percent of words that are missing from vocabulary: 6.17%\n"
     ]
    }
   ],
   "source": [
    "# Find the number of words that are missing from CN, and are used more than our threshold.\n",
    "missing_words = 0\n",
    "threshold = 10\n",
    "\n",
    "for word, count in word_counts.items():\n",
    "    if count > threshold:\n",
    "        if word not in embeddings_index:\n",
    "            missing_words += 1\n",
    "            \n",
    "missing_ratio = round(missing_words/len(word_counts),4)*100\n",
    "            \n",
    "print(\"Number of words missing from CN:\", missing_words)\n",
    "print(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words: 9961\n",
      "Number of words we will use: 7777\n",
      "Percent of words we will use: 78.07%\n"
     ]
    }
   ],
   "source": [
    "# Limit the vocab that we will use to words that appear ≥ threshold or are in GloVe\n",
    "\n",
    "#dictionary to convert words to integers\n",
    "vocab_to_int = {} \n",
    "\n",
    "value = 0\n",
    "for word, count in word_counts.items():\n",
    "    if count >= threshold or word in embeddings_index:\n",
    "        vocab_to_int[word] = value\n",
    "        value += 1\n",
    "\n",
    "# Special tokens that will be added to our vocab\n",
    "codes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]   \n",
    "\n",
    "# Add codes to vocab\n",
    "for code in codes:\n",
    "    vocab_to_int[code] = len(vocab_to_int)\n",
    "\n",
    "# Dictionary to convert integers to words\n",
    "int_to_vocab = {}\n",
    "for word, value in vocab_to_int.items():\n",
    "    int_to_vocab[value] = word\n",
    "\n",
    "usage_ratio = round(len(vocab_to_int) / len(word_counts),4)*100\n",
    "\n",
    "print(\"Total number of unique words:\", len(word_counts))\n",
    "print(\"Number of words we will use:\", len(vocab_to_int))\n",
    "print(\"Percent of words we will use: {}%\".format(usage_ratio))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7777\n"
     ]
    }
   ],
   "source": [
    "# Need to use 300 for embedding dimensions to match CN's vectors.\n",
    "embedding_dim = 300\n",
    "nb_words = len(vocab_to_int)\n",
    "\n",
    "# Create matrix with default values of zero\n",
    "word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
    "for word, i in vocab_to_int.items():\n",
    "    if word in embeddings_index:\n",
    "        word_embedding_matrix[i] = embeddings_index[word]\n",
    "    else:\n",
    "        # If word not in CN, create a random embedding for it\n",
    "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "        embeddings_index[word] = new_embedding\n",
    "        word_embedding_matrix[i] = new_embedding\n",
    "\n",
    "# Check if value matches len(vocab_to_int)\n",
    "print(len(word_embedding_matrix))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_ints(text, word_count, unk_count, eos=False):\n",
    "    '''Convert words in text to an integer.\n",
    "       If word is not in vocab_to_int, use UNK's integer.\n",
    "       Total the number of words and UNKs.\n",
    "       Add EOS token to the end of texts'''\n",
    "    ints = []\n",
    "    for sentence in text:\n",
    "        sentence_ints = []\n",
    "        for word in sentence.split():\n",
    "            word_count += 1\n",
    "            if word in vocab_to_int:\n",
    "                sentence_ints.append(vocab_to_int[word])\n",
    "            else:\n",
    "                sentence_ints.append(vocab_to_int[\"<UNK>\"])\n",
    "                unk_count += 1\n",
    "        if eos:\n",
    "            sentence_ints.append(vocab_to_int[\"<EOS>\"])\n",
    "        ints.append(sentence_ints)\n",
    "    return ints, word_count, unk_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in headlines: 1919094\n",
      "Total number of UNKs in headlines: 5155\n",
      "Percent of words that are UNK: 0.27%\n"
     ]
    }
   ],
   "source": [
    "# Apply convert_to_ints to clean_summaries and clean_texts\n",
    "word_count = 0\n",
    "unk_count = 0\n",
    "\n",
    "int_summaries, word_count, unk_count = convert_to_ints(clean_summaries, word_count, unk_count)\n",
    "int_texts, word_count, unk_count = convert_to_ints(clean_texts, word_count, unk_count, eos=True)\n",
    "\n",
    "unk_percent = round(unk_count/word_count,4)*100\n",
    "\n",
    "print(\"Total number of words in headlines:\", word_count)\n",
    "print(\"Total number of UNKs in headlines:\", unk_count)\n",
    "print(\"Percent of words that are UNK: {}%\".format(unk_percent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lengths(text):\n",
    "    '''Create a data frame of the sentence lengths from a text'''\n",
    "    lengths = []\n",
    "    for sentence in text:\n",
    "        lengths.append(len(sentence))\n",
    "    return pd.DataFrame(lengths, columns=['counts'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries:\n",
      "             counts\n",
      "count  27284.000000\n",
      "mean      19.013671\n",
      "std       21.593241\n",
      "min        1.000000\n",
      "25%        7.000000\n",
      "50%       11.000000\n",
      "75%       22.000000\n",
      "max      294.000000\n",
      "\n",
      "Texts:\n",
      "             counts\n",
      "count  27284.000000\n",
      "mean      52.324036\n",
      "std       58.845538\n",
      "min        3.000000\n",
      "25%       20.000000\n",
      "50%       29.000000\n",
      "75%       57.000000\n",
      "max     1220.000000\n"
     ]
    }
   ],
   "source": [
    "lengths_summaries = create_lengths(int_summaries)\n",
    "lengths_texts = create_lengths(int_texts)\n",
    "\n",
    "print(\"Summaries:\")\n",
    "print(lengths_summaries.describe())\n",
    "print()\n",
    "print(\"Texts:\")\n",
    "print(lengths_texts.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unk_counter(sentence):\n",
    "    '''Counts the number of time UNK appears in a sentence.'''\n",
    "    unk_count = 0\n",
    "    for word in sentence:\n",
    "        if word == vocab_to_int[\"<UNK>\"]:\n",
    "            unk_count += 1\n",
    "    return unk_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_tmp = 1\n",
    "all_lengths_tmp = []\n",
    "for length in lengths_texts.counts:\n",
    "#     print(length)\n",
    "    all_lengths_tmp.append(length)\n",
    "    if length > max_length_tmp:\n",
    "        max_length_tmp = length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] max length:  1220\n",
      "[info] avg length:  52.3240360650931\n"
     ]
    }
   ],
   "source": [
    "print(\"[info] max length: \", max_length_tmp)\n",
    "print(\"[info] avg length: \", np.mean(all_lengths_tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_tmp = 1\n",
    "all_lengths_tmp = []\n",
    "for length in lengths_summaries.counts:\n",
    "#     print(length)\n",
    "    all_lengths_tmp.append(length)\n",
    "    if length > max_length_tmp:\n",
    "        max_length_tmp = length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] max length summaries:  294\n",
      "[info] avg length summaries:  19.013671015980062\n"
     ]
    }
   ],
   "source": [
    "print(\"[info] max length summaries: \", max_length_tmp)\n",
    "print(\"[info] avg length summaries: \", np.mean(all_lengths_tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25759\n",
      "25759\n"
     ]
    }
   ],
   "source": [
    "# Sort the summaries and texts by the length of the texts, shortest to longest\n",
    "# Limit the length of summaries and texts based on the min and max ranges.\n",
    "# Remove notes that include too many UNKs\n",
    "\n",
    "sorted_summaries = []\n",
    "sorted_texts = []\n",
    "max_text_length = 1220\n",
    "max_summary_length = 294\n",
    "min_length = 2\n",
    "unk_text_limit = 1\n",
    "unk_summary_limit = 0\n",
    "\n",
    "for length in range(min(lengths_texts.counts), max_text_length): \n",
    "    for count, words in enumerate(int_summaries):\n",
    "        if (len(int_summaries[count]) >= min_length and\n",
    "            len(int_summaries[count]) <= max_summary_length and\n",
    "            len(int_texts[count]) >= min_length and\n",
    "            unk_counter(int_summaries[count]) <= unk_summary_limit and\n",
    "            unk_counter(int_texts[count]) <= unk_text_limit and\n",
    "            length == len(int_texts[count])\n",
    "           ):\n",
    "            sorted_summaries.append(int_summaries[count])\n",
    "            sorted_texts.append(int_texts[count])\n",
    "        \n",
    "# Compare lengths to ensure they match\n",
    "print(len(sorted_summaries))\n",
    "print(len(sorted_texts))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    summary_length = tf.placeholder(tf.int32, (None,), name='summary_length')\n",
    "    max_summary_length = tf.reduce_max(summary_length, name='max_dec_len')\n",
    "    text_length = tf.placeholder(tf.int32, (None,), name='text_length')\n",
    "\n",
    "    return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_encoding_input(target_data, vocab_to_int, batch_size):  \n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1]) # slice it to target_data[0:batch_size, 0: -1]\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
    "\n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the encoding layers\n",
    "bidirectional_dynamic_rnn use tf.variable_scope so that variables are reused with each layer\n",
    "\n",
    "parameters\n",
    "\n",
    "    -rnn_size: The number of units in the LSTM cell\n",
    "    -sequence_length: size [batch_size], containing the actual lengths for each of the sequences in the batch\n",
    "    -num_layers: number of bidirectional RNN layer\n",
    "    -rnn_inputs: number of bidirectional RNN layer\n",
    "    -keep_prob: RNN dropout input keep probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob):\n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('encoder_{}'.format(layer)):\n",
    "            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n",
    "                                                    input_keep_prob = keep_prob)\n",
    "\n",
    "            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n",
    "                                                    input_keep_prob = keep_prob)\n",
    "\n",
    "            enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
    "                                                                    cell_bw, \n",
    "                                                                    rnn_inputs,\n",
    "                                                                    sequence_length,\n",
    "                                                                    dtype=tf.float32)\n",
    "            enc_output = tf.concat(enc_output,2)\n",
    "            # original code is missing this line below, that is how we connect layers \n",
    "            # by feeding the current layer's output to next layer's input\n",
    "            rnn_inputs = enc_output\n",
    "    return enc_output, enc_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the training decoding layer\n",
    "parameters\n",
    "\n",
    "    -dec_embed_input: output of embedding_lookup for a batch of inputs\n",
    "    -summary_length: length of each padded summary sequences in batch, since padded, all lengths should be same number\n",
    "    -dec_cell: the decoder RNN cells' output with attention wapper\n",
    "    -output_layer: fully connected layer to apply to the RNN output\n",
    "    -vocab_size: vocabulary size i.e. len(vocab_to_int)+1\n",
    "    -max_summary_length: the maximum length of a summary in a batch\n",
    "    -batch_size: number of input sequences in a batch\n",
    "\n",
    "Three components\n",
    "\n",
    "    -TraingHelper reads a sequence of integers from the encoding layer.\n",
    "    -BasicDecoder processes the sequence with the decoding cell, and an output layer, which is a fully connected layer. initial_state set to zero state.\n",
    "    -dynamic_decode creates our outputs that will be used for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_decoding_layer(dec_embed_input, summary_length, dec_cell, output_layer,\n",
    "                            vocab_size, max_summary_length,batch_size):\n",
    "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
    "                                                        sequence_length=summary_length,\n",
    "                                                        time_major=False)\n",
    "\n",
    "    training_decoder = tf.contrib.seq2seq.BasicDecoder(cell=dec_cell,\n",
    "                                                       helper=training_helper,\n",
    "                                                       initial_state=dec_cell.zero_state(dtype=tf.float32, batch_size=batch_size),\n",
    "                                                       output_layer = output_layer)\n",
    "\n",
    "    training_logits = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                                           output_time_major=False,\n",
    "                                                           impute_finished=True,\n",
    "                                                           maximum_iterations=max_summary_length)\n",
    "    return training_logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create infer decoding layer\n",
    "parameters\n",
    "\n",
    "    -embeddings: the CN's word_embedding_matrix\n",
    "    -start_token: the id of <GO>\n",
    "    -end_token: the id of <EOS>\n",
    "    -dec_cell: the decoder RNN cells' output with attention wapper\n",
    "    -output_layer: fully connected layer to apply to the RNN output\n",
    "    -max_summary_length: the maximum length of a summary in a batch\n",
    "    -batch_size: number of input sequences in a batch\n",
    "\n",
    "GreedyEmbeddingHelper argument start_tokens: int32 vector shaped [batch_size], the start tokens.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, output_layer,\n",
    "                             max_summary_length, batch_size):\n",
    "    '''Create the inference logits'''\n",
    "    \n",
    "    start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "    \n",
    "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
    "                                                                start_tokens,\n",
    "                                                                end_token)\n",
    "                \n",
    "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                        inference_helper,\n",
    "                                                        dec_cell.zero_state(dtype=tf.float32, batch_size=batch_size),\n",
    "                                                        output_layer)\n",
    "                \n",
    "    inference_logits = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                                            output_time_major=False,\n",
    "                                                            impute_finished=True,\n",
    "                                                            maximum_iterations=max_summary_length)\n",
    "    \n",
    "    return inference_logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Decoding layer\n",
    "3 parts: decoding cell, attention, and getting our logits.\n",
    "\n",
    "## Decoding Cell:\n",
    "Just a two layer LSTM with dropout.\n",
    "\n",
    "### Attention:\n",
    "Using Bhadanau, since trains faster than Luong.\n",
    "\n",
    "### AttentionWrapper applies the attention mechanism to our decoding cell.\n",
    "\n",
    "parameters\n",
    "\n",
    "    -dec_embed_input: output of embedding_lookup for a batch of inputs\n",
    "    -embeddings: the CN's word_embedding_matrix\n",
    "    -enc_output: encoder layer output, containing the forward and the backward rnn output\n",
    "    -enc_state: encoder layer state, a tuple containing the forward and the backward final states of bidirectional rnn.\n",
    "    -vocab_size: vocabulary size i.e. len(vocab_to_int)+1\n",
    "    -text_length: the actual lengths for each of the input text sequences in the batch\n",
    "    -summary_length: the actual lengths for each of the input summary sequences in the batch\n",
    "    -max_summary_length: the maximum length of a summary in a batch\n",
    "    -rnn_size: The number of units in the LSTM cell\n",
    "    -vocab_to_int: vocab_to_int the dictionary\n",
    "    -keep_prob: RNN dropout input keep probability\n",
    "    -batch_size: number of input sequences in a batch\n",
    "    -num_layers: number of decoder RNN layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_cell(lstm_size, keep_prob):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    return tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob = keep_prob)\n",
    "\n",
    "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length,\n",
    "                   max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):\n",
    "    '''Create the decoding cell and attention for the training and inference decoding layers'''\n",
    "    dec_cell = tf.contrib.rnn.MultiRNNCell([lstm_cell(rnn_size, keep_prob) for _ in range(num_layers)])\n",
    "    output_layer = Dense(vocab_size,kernel_initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.1))\n",
    "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n",
    "                                                     enc_output,\n",
    "                                                     text_length,\n",
    "                                                     normalize=False,\n",
    "                                                     name='BahdanauAttention')\n",
    "    dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell,attn_mech,rnn_size)\n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        training_logits = training_decoding_layer(dec_embed_input,summary_length,dec_cell,\n",
    "                                                  output_layer,\n",
    "                                                  vocab_size,\n",
    "                                                  max_summary_length,\n",
    "                                                  batch_size)\n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        inference_logits = inference_decoding_layer(embeddings,\n",
    "                                                    vocab_to_int['<GO>'],\n",
    "                                                    vocab_to_int['<EOS>'],\n",
    "                                                    dec_cell,\n",
    "                                                    output_layer,\n",
    "                                                    max_summary_length,\n",
    "                                                    batch_size)\n",
    "    return training_logits, inference_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, \n",
    "                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size):\n",
    "    '''Use the previous functions to create the training and inference logits'''\n",
    "    \n",
    "    # Use Numberbatch's embeddings and the newly created ones as our embeddings\n",
    "    embeddings = word_embedding_matrix\n",
    "    enc_embed_input = tf.nn.embedding_lookup(embeddings, input_data)\n",
    "    enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)\n",
    "    dec_input = process_encoding_input(target_data, vocab_to_int, batch_size) #shape=(batch_size, senquence length) each seq start with index of<GO>\n",
    "    dec_embed_input = tf.nn.embedding_lookup(embeddings, dec_input)\n",
    "    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n",
    "                                                        embeddings,\n",
    "                                                        enc_output,\n",
    "                                                        enc_state, \n",
    "                                                        vocab_size, \n",
    "                                                        text_length, \n",
    "                                                        summary_length, \n",
    "                                                        max_summary_length,\n",
    "                                                        rnn_size, \n",
    "                                                        vocab_to_int, \n",
    "                                                        keep_prob, \n",
    "                                                        batch_size,\n",
    "                                                        num_layers)\n",
    "    return training_logits, inference_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(summaries, texts, batch_size):\n",
    "    \"\"\"Batch summaries, texts, and the lengths of their sentences together\"\"\"\n",
    "    for batch_i in range(0, len(texts)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        summaries_batch = summaries[start_i:start_i + batch_size]\n",
    "        texts_batch = texts[start_i:start_i + batch_size]\n",
    "        pad_summaries_batch = np.array(pad_sentence_batch(summaries_batch))\n",
    "        pad_texts_batch = np.array(pad_sentence_batch(texts_batch))\n",
    "        \n",
    "        # Need the lengths for the _lengths parameters\n",
    "        pad_summaries_lengths = []\n",
    "        for summary in pad_summaries_batch:\n",
    "            pad_summaries_lengths.append(len(summary))\n",
    "        \n",
    "        pad_texts_lengths = []\n",
    "        for text in pad_texts_batch:\n",
    "            pad_texts_lengths.append(len(text))\n",
    "        \n",
    "        yield pad_summaries_batch, pad_texts_batch, pad_summaries_lengths, pad_texts_lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'<PAD>' has id: 7774\n",
      "pad summaries batch samples:\n",
      "\r",
      " [[   0    1  177 7774]\n",
      " [   0    1  177 7774]\n",
      " [   0    1  177 7774]\n",
      " [   0   64  177 7774]\n",
      " [   0  199   10   11]]\n"
     ]
    }
   ],
   "source": [
    "print(\"'<PAD>' has id: {}\".format(vocab_to_int['<PAD>']))\n",
    "sorted_summaries_samples = sorted_summaries[7:50]\n",
    "sorted_texts_samples = sorted_texts[7:50]\n",
    "pad_summaries_batch_samples, pad_texts_batch_samples, pad_summaries_lengths_samples, pad_texts_lengths_samples = next(get_batches(\n",
    "    sorted_summaries_samples, sorted_texts_samples, 5))\n",
    "print(\"pad summaries batch samples:\\n\\r {}\".format(pad_summaries_batch_samples))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Hyperparameters\n",
    "epochs = 1\n",
    "batch_size = 64\n",
    "# batch_size = 10\n",
    "batch_size = 100\n",
    "rnn_size = 256\n",
    "num_layers = 2\n",
    "learning_rate = 0.005\n",
    "keep_probability = 0.95\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph is built.\n",
      "./graph\n"
     ]
    }
   ],
   "source": [
    "# Build the graph\n",
    "train_graph = tf.Graph()\n",
    "# Set the graph to default to ensure that it is ready for training\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    # Load the model inputs    \n",
    "    input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()\n",
    "\n",
    "    # Create the training and inference logits\n",
    "    training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                      targets, \n",
    "                                                      keep_prob,   \n",
    "                                                      text_length,\n",
    "                                                      summary_length,\n",
    "                                                      max_summary_length,\n",
    "                                                      len(vocab_to_int)+1,\n",
    "                                                      rnn_size, \n",
    "                                                      num_layers, \n",
    "                                                      vocab_to_int,\n",
    "                                                      batch_size)\n",
    "    \n",
    "    # Create tensors for the training logits and inference logits\n",
    "    training_logits = tf.identity(training_logits[0].rnn_output, 'logits')\n",
    "    inference_logits = tf.identity(inference_logits[0].sample_id, name='predictions')\n",
    "    \n",
    "    # Create the weights for sequence_loss, the sould be all True across since each batch is padded\n",
    "    masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "print(\"Graph is built.\")\n",
    "graph_location = \"./graph\"\n",
    "print(graph_location)\n",
    "train_writer = tf.summary.FileWriter(graph_location)\n",
    "train_writer.add_graph(train_graph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shortest text length: 3\n",
      "The longest text length: 18\n"
     ]
    }
   ],
   "source": [
    "# Subset the data for training\n",
    "# start = 200000\n",
    "start = 0\n",
    "end = start + 5000\n",
    "sorted_summaries_short = sorted_summaries[start:end]\n",
    "sorted_texts_short = sorted_texts[start:end]\n",
    "print(\"The shortest text length:\", len(sorted_texts_short[0]))\n",
    "print(\"The longest text length:\",len(sorted_texts_short[-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpfs/data/ildproject-share/modelparams/seq2seq/best_model_2018-09-03_13_02_36.ckpt\n"
     ]
    }
   ],
   "source": [
    "ckpt_text = \"gpfs/data/ildproject-share/modelparams/seq2seq/best_model_%s.ckpt\" % (str(datetime.datetime.now()).split('.')[0].replace(' ','_').replace(':','_'))\n",
    "print(ckpt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/1 Batch    1/50 - Loss: 13.048, Seconds: 1.47\n",
      "Epoch   1/1 Batch    2/50 - Loss:  6.000, Seconds: 2.42\n",
      "Epoch   1/1 Batch    3/50 - Loss:  5.486, Seconds: 2.38\n",
      "Epoch   1/1 Batch    4/50 - Loss:  2.929, Seconds: 2.18\n",
      "Epoch   1/1 Batch    5/50 - Loss:  1.968, Seconds: 2.51\n",
      "Epoch   1/1 Batch    6/50 - Loss:  2.720, Seconds: 1.87\n",
      "Epoch   1/1 Batch    7/50 - Loss:  2.012, Seconds: 2.34\n",
      "Epoch   1/1 Batch    8/50 - Loss:  2.782, Seconds: 1.78\n",
      "Epoch   1/1 Batch    9/50 - Loss:  0.972, Seconds: 5.48\n",
      "Epoch   1/1 Batch   10/50 - Loss:  0.905, Seconds: 5.15\n",
      "Epoch   1/1 Batch   11/50 - Loss:  2.083, Seconds: 2.39\n",
      "Epoch   1/1 Batch   12/50 - Loss:  2.357, Seconds: 2.17\n",
      "Epoch   1/1 Batch   13/50 - Loss:  0.765, Seconds: 6.30\n",
      "Epoch   1/1 Batch   14/50 - Loss:  1.650, Seconds: 2.60\n",
      "Epoch   1/1 Batch   15/50 - Loss:  1.174, Seconds: 3.91\n",
      "Average loss for this update: 3.123\n",
      "New Record!\n",
      "Epoch   1/1 Batch   16/50 - Loss:  1.272, Seconds: 3.82\n",
      "Epoch   1/1 Batch   17/50 - Loss:  1.226, Seconds: 4.08\n",
      "Epoch   1/1 Batch   18/50 - Loss:  1.907, Seconds: 2.59\n",
      "Epoch   1/1 Batch   19/50 - Loss:  1.510, Seconds: 2.74\n",
      "Epoch   1/1 Batch   20/50 - Loss:  0.923, Seconds: 5.19\n",
      "Epoch   1/1 Batch   21/50 - Loss:  1.620, Seconds: 2.97\n",
      "Epoch   1/1 Batch   22/50 - Loss:  0.991, Seconds: 4.48\n",
      "Epoch   1/1 Batch   23/50 - Loss:  1.797, Seconds: 2.54\n",
      "Epoch   1/1 Batch   24/50 - Loss:  1.415, Seconds: 3.53\n",
      "Epoch   1/1 Batch   25/50 - Loss:  1.612, Seconds: 3.42\n",
      "Epoch   1/1 Batch   26/50 - Loss:  1.178, Seconds: 4.25\n",
      "Epoch   1/1 Batch   27/50 - Loss:  1.469, Seconds: 3.42\n",
      "Epoch   1/1 Batch   28/50 - Loss:  1.449, Seconds: 3.72\n",
      "Epoch   1/1 Batch   29/50 - Loss:  2.071, Seconds: 2.47\n",
      "Epoch   1/1 Batch   30/50 - Loss:  1.894, Seconds: 2.72\n",
      "Average loss for this update: 1.489\n",
      "New Record!\n",
      "Epoch   1/1 Batch   31/50 - Loss:  1.527, Seconds: 3.25\n",
      "Epoch   1/1 Batch   32/50 - Loss:  1.932, Seconds: 2.84\n",
      "Epoch   1/1 Batch   33/50 - Loss:  1.384, Seconds: 3.53\n",
      "Epoch   1/1 Batch   34/50 - Loss:  1.640, Seconds: 3.41\n",
      "Epoch   1/1 Batch   35/50 - Loss:  1.727, Seconds: 3.20\n",
      "Epoch   1/1 Batch   36/50 - Loss:  0.952, Seconds: 4.92\n",
      "Epoch   1/1 Batch   37/50 - Loss:  1.969, Seconds: 2.87\n",
      "Epoch   1/1 Batch   38/50 - Loss:  1.260, Seconds: 4.53\n",
      "Epoch   1/1 Batch   39/50 - Loss:  0.577, Seconds: 9.00\n",
      "Epoch   1/1 Batch   40/50 - Loss:  1.950, Seconds: 2.91\n",
      "Epoch   1/1 Batch   41/50 - Loss:  1.266, Seconds: 4.22\n",
      "Epoch   1/1 Batch   42/50 - Loss:  1.314, Seconds: 4.06\n",
      "Epoch   1/1 Batch   43/50 - Loss:  1.540, Seconds: 3.44\n",
      "Epoch   1/1 Batch   44/50 - Loss:  2.221, Seconds: 2.47\n",
      "Epoch   1/1 Batch   45/50 - Loss:  1.533, Seconds: 4.28\n",
      "Average loss for this update: 1.519\n",
      "No Improvement.\n",
      "Stopping Training.\n"
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "learning_rate_decay = 0.95\n",
    "min_learning_rate = 0.0005\n",
    "display_step = 1 # Check training loss after every 20 batches\n",
    "stop_early = 0 \n",
    "stop = 1 # If the update loss does not decrease in 3 consecutive update checks, stop training\n",
    "per_epoch = 3 # Make 3 update checks per epoch\n",
    "update_check = (len(sorted_texts_short)//batch_size//per_epoch)-1\n",
    "\n",
    "update_loss = 0 \n",
    "batch_loss = 0\n",
    "summary_update_loss = [] # Record the update losses for saving improvements in the model\n",
    "\n",
    "# ckpt_text = \"gpfs/data/ildproject-share/modelparams/seq2seq/best_model_%s.ckpt\" % (str(datetime.datetime.now()).split('.')[0].replace(' ','_').replace(':','_'))\n",
    "ckpt_text = \"./best_model.ckpt\" \n",
    "checkpoint = ckpt_text \n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # If we want to continue training a previous session\n",
    "    #loader = tf.train.import_meta_graph(\"./\" + checkpoint + '.meta')\n",
    "    #loader.restore(sess, checkpoint)\n",
    "    \n",
    "    for epoch_i in range(1, epochs+1):\n",
    "        update_loss = 0\n",
    "        batch_loss = 0\n",
    "        for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\n",
    "                get_batches(sorted_summaries_short, sorted_texts_short, batch_size)):\n",
    "            start_time = time.time()\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: texts_batch,\n",
    "                 targets: summaries_batch,\n",
    "                 lr: learning_rate,\n",
    "                 summary_length: summaries_lengths,\n",
    "                 text_length: texts_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "\n",
    "            batch_loss += loss\n",
    "            update_loss += loss\n",
    "            end_time = time.time()\n",
    "            batch_time = end_time - start_time\n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                      .format(epoch_i,\n",
    "                              epochs, \n",
    "                              batch_i, \n",
    "                              len(sorted_texts_short) // batch_size, \n",
    "                              batch_loss / display_step, \n",
    "                              batch_time*display_step))\n",
    "                batch_loss = 0\n",
    "\n",
    "            if batch_i % update_check == 0 and batch_i > 0:\n",
    "                print(\"Average loss for this update:\", round(update_loss/update_check,3))\n",
    "                summary_update_loss.append(update_loss)\n",
    "                \n",
    "                # If the update loss is at a new minimum, save the model\n",
    "                if update_loss <= min(summary_update_loss):\n",
    "                    print('New Record!') \n",
    "                    stop_early = 0\n",
    "                    saver = tf.train.Saver() \n",
    "                    saver.save(sess, checkpoint)\n",
    "\n",
    "                else:\n",
    "                    print(\"No Improvement.\")\n",
    "                    stop_early += 1\n",
    "                    if stop_early == stop:\n",
    "                        break\n",
    "                update_loss = 0\n",
    "            \n",
    "                    \n",
    "        # Reduce learning rate, but not below its minimum value\n",
    "        learning_rate *= learning_rate_decay\n",
    "        if learning_rate < min_learning_rate:\n",
    "            learning_rate = min_learning_rate\n",
    "        \n",
    "        if stop_early == stop:\n",
    "            print(\"Stopping Training.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summaries of our own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "list_of_files = glob.glob('/gpfs/data/ildproject-share/modelparams/seq2seq/*') # * means all if need specific format then *.csv\n",
    "latest_file = max(list_of_files, key=os.path.getctime)\n",
    "ckpt_text = print(latest_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_seq(text):\n",
    "    '''Prepare the text for the model'''\n",
    "    \n",
    "    text = clean_text(text)\n",
    "    return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in text.split()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2728: DtypeWarning: Columns (18,20,35,54,63,66,163,175,178,179,181,182,184,185,187,188,190,191,193,194,196,197,199,200,202,203,205,206,208,209,211,212,214,215,217,218,220,221,223,224,226,227,229,230,232,233,235,236,238,239,241,242,244,245,247,248,250,251,253,254,256,257,259,260,262,263,265,266,268,269,271,272,274,275,277,278,280,281,283,284,286,287,289,290,292,293,295,296,298,299,301,302,304,305,307,308,310,311,313,314,316,317,319,320,322,323) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "test_notes = pd.read_csv(\"Z:/final_data/cohort1_final_data.csv\")\n",
    "# notes = pd.read_csv(\"gpfs/data/ildproject-share/final_data/cohort1_final_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>findings</th>\n",
       "      <th>impressions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Left-sided AICD are unchanged. Sternal wires a...</td>\n",
       "      <td>No acute cardiopulmonary process.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Increasing right basilar opacity highly sugges...</td>\n",
       "      <td>Increasing right basilar opacity highly sugges...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            findings  \\\n",
       "0                                                NaN   \n",
       "1  Left-sided AICD are unchanged. Sternal wires a...   \n",
       "2  Increasing right basilar opacity highly sugges...   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                         impressions  \n",
       "0                                                NaN  \n",
       "1                  No acute cardiopulmonary process.  \n",
       "2  Increasing right basilar opacity highly sugges...  \n",
       "3                                                NaN  \n",
       "4                                                NaN  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_notes = test_notes[['findings','impressions']]\n",
    "test_notes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "findings       40179\n",
       "impressions    40018\n",
       "dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_notes.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove null values and unneeded features\n",
    "test_notes = test_notes.dropna()\n",
    "# notes = notes.drop(['Unnamed: 0','ScanType','ScanDate','ClinInfo','Technique','Comparison',\n",
    "#                         'ElecSig','Patient','RawText'], 1)\n",
    "test_notes = test_notes.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>findings</th>\n",
       "      <th>impressions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Left-sided AICD are unchanged. Sternal wires a...</td>\n",
       "      <td>No acute cardiopulmonary process.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Increasing right basilar opacity highly sugges...</td>\n",
       "      <td>Increasing right basilar opacity highly sugges...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unremarkable cardiac and mediastinal silhouett...</td>\n",
       "      <td>Mild diffuse interstitial disease, previously ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Increase in diffuse pulmonary opacities. Small...</td>\n",
       "      <td>Increase in diffuse pulmonary opacities which ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Diffuse nonspecific interstitial opacity, not ...</td>\n",
       "      <td>Diffuse nonspecific interstitial disease of un...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            findings  \\\n",
       "0  Left-sided AICD are unchanged. Sternal wires a...   \n",
       "1  Increasing right basilar opacity highly sugges...   \n",
       "2  Unremarkable cardiac and mediastinal silhouett...   \n",
       "3  Increase in diffuse pulmonary opacities. Small...   \n",
       "4  Diffuse nonspecific interstitial opacity, not ...   \n",
       "\n",
       "                                         impressions  \n",
       "0                  No acute cardiopulmonary process.  \n",
       "1  Increasing right basilar opacity highly sugges...  \n",
       "2  Mild diffuse interstitial disease, previously ...  \n",
       "3  Increase in diffuse pulmonary opacities which ...  \n",
       "4  Diffuse nonspecific interstitial disease of un...  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_notes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] dimensions of notes (27284, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"[info] dimensions of notes\",test_notes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries are complete.\n",
      "Texts are complete.\n"
     ]
    }
   ],
   "source": [
    "# cleaned test\n",
    "# Clean the summaries and texts\n",
    "test_clean_summaries = []\n",
    "for summary in test_notes.impressions:\n",
    "    test_clean_summaries.append(clean_text(summary, remove_stopwords=False))\n",
    "print(\"Summaries are complete.\")\n",
    "\n",
    "test_clean_texts = []\n",
    "for text in test_notes.findings:\n",
    "    test_clean_texts.append(clean_text(text))\n",
    "print(\"Texts are complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27284"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_clean_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_texts = [text_to_seq(input_sentence) for input_sentence in test_notes.impressions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3],\n",
       " [4, 5, 6, 7, 8, 9, 11, 13],\n",
       " [14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  21,\n",
       "  22,\n",
       "  24,\n",
       "  21,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  30,\n",
       "  31,\n",
       "  32,\n",
       "  33,\n",
       "  17,\n",
       "  35,\n",
       "  37,\n",
       "  38,\n",
       "  39,\n",
       "  40],\n",
       " [41, 15, 42, 43, 45, 46, 42, 47, 42, 48, 42, 49, 7773, 51, 52],\n",
       " [15, 53, 16, 17, 54, 55, 56, 57, 58, 59]]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = test_texts[0:5]\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                    No acute cardiopulmonary process.\n",
       "1    Increasing right basilar opacity highly sugges...\n",
       "2    Mild diffuse interstitial disease, previously ...\n",
       "3    Increase in diffuse pulmonary opacities which ...\n",
       "4    Diffuse nonspecific interstitial disease of un...\n",
       "Name: impressions, dtype: object"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sentences = test_notes.impressions[0:5]\n",
    "input_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(input_sentences))\n",
    "# generagte_summary_length =  [3,2]\n",
    "generagte_summary_length =  100;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_sentences=[\"The coffee tasted great and was at such a good price! I highly recommend this to everyone!\",\n",
    "#                \"love individual oatmeal cups found years ago sam quit selling sound big lots quit selling found target expensive buy individually trilled get entire case time go anywhere need water microwave spoon know quaker flavor packets\"]\n",
    "# generagte_summary_length =  [3,2]\n",
    "\n",
    "# texts = [text_to_seq(input_sentence) for input_sentence in input_sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./best_model.ckpt\n",
      "[ 165   42 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774]\n",
      "- Review:\n",
      " No acute cardiopulmonary process.\n",
      "- Summary:\n",
      " cardiomegaly pulmonary\n",
      "\n",
      "\n",
      "[   0    0 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774]\n",
      "- Review:\n",
      " Increasing right basilar opacity highly suggestive of pneumonia or aspiration.\n",
      "- Summary:\n",
      " no no\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774]\n",
      "- Review:\n",
      " Mild diffuse interstitial disease, previously characterized by CT scan. As described in the CT report, the differential diagnosis includes smoking related lung disease (if the history is consistent) as well as hypersensitivity pneumonitis.\n",
      "- Summary:\n",
      " no no\n",
      "\n",
      "\n",
      "[   0    0 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774]\n",
      "- Review:\n",
      " Increase in diffuse pulmonary opacities which may represent pulmonary edema, pulmonary hemorrhage, pulmonary alveolar proteinosis, and/or atypical infection.\n",
      "- Summary:\n",
      " no no\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774 7774\n",
      " 7774 7774 7774 7774 7774 7774]\n",
      "- Review:\n",
      "\r",
      " Diffuse nonspecific interstitial disease of uncertain etiology, without appreciable interval change.\n",
      "- Summary:\n",
      "\r",
      " no no\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# input_sentences=[\"The coffee tasted great and was at such a good price! I highly recommend this to everyone!\",\n",
    "#                \"love individual oatmeal cups found years ago sam quit selling sound big lots quit selling found target expensive buy individually trilled get entire case time go anywhere need water microwave spoon know quaker flavor packets\"]\n",
    "# generagte_summary_length =  [3,2]\n",
    "\n",
    "# texts = [text_to_seq(input_sentence) for input_sentence in input_sentences]\n",
    "checkpoint = ckpt_text\n",
    "if type(generagte_summary_length) is list:\n",
    "    if len(input_sentences)!=len(generagte_summary_length):\n",
    "        raise Exception(\"[Error] makeSummaries parameter generagte_summary_length must be same length as input_sentences or an integer\")\n",
    "    generagte_summary_length_list = generagte_summary_length\n",
    "else:\n",
    "    generagte_summary_length_list = [generagte_summary_length] * len(texts)\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    loader.restore(sess, checkpoint)\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
    "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "    #Multiply by batch_size to match the model's input parameters\n",
    "    for i, text in enumerate(texts):\n",
    "        generagte_summary_length = generagte_summary_length_list[i]\n",
    "        answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
    "                                          summary_length: [generagte_summary_length], #summary_length: [np.random.randint(5,8)], \n",
    "                                          text_length: [len(text)]*batch_size,\n",
    "                                          keep_prob: 1.0})[0]\n",
    "        print(answer_logits)\n",
    "        # Remove the padding from the summaries\n",
    "        pad = vocab_to_int[\"<PAD>\"] \n",
    "        print('- Review:\\n\\r {}'.format(input_sentences[i]))\n",
    "        print('- Summary:\\n\\r {}\\n\\r\\n\\r'.format(\" \".join([int_to_vocab[i] for i in answer_logits if i != pad])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
